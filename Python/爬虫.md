# request&beautifulsoup

```python
from bs4 import BeautifulSoup
# BeautifulSoup 的功能就是将文本内容通过标签的嵌套的方式，转换成对象内部嵌套对象的方式
# 使得在后面的取值时，可以通过对象的方式(.取值)
import requests


request_data = requests.get('URL')

# request_data.encoding = "utf-8"  # 指定返回数据的编码格式
request_data.encoding = request_data.apparent_encoding
# 上面的流程指的是，get时接收到的数据用什么编码格式我就用什么方式解码

content = request_data.content  # 得到返回的二进制数据
status_code = request_data.status_code  # 得到状态码

soup = BeautifulSoup('html内容（变成str）', features='html.parser')
# features指定的内容是告诉BeautifulSoup模块我现在需要转换为对象的内容是html

soup.find('div')  # 找soup里面第一个， 返回结果为一个对象 没有找到是返回值为none
soup.find(id='id值')
# soup.name  返回标签名
soup.find('div', id='id值')

soup.find(name=None, attrs={}, recursive=True, text=None)
# 源码中的recursive是指是否允许递归查找，如果改为false，那么查找时相当于只查找子标签


print(soup.find('div'))  # 打印时得到的内容被内部的__str__方法改写了，输出类似于一个标签

# 当用一个变量接收返回值，那么可以使用返回的对象继续深入寻找,即其可以链式寻找
soup.find('div').find("div").find("div")


# 找soup内部所有,参数和使用和find相同，返回对象是一个列表中有想要搜索的对象
soup.find_all('div')

soup.select("#id1")  # 使用select方法，可以使用名称筛选方式，内部转换
soup.select_one(".class1")

obj = soup.find()
text = obj.text  # 得到其内的文本数据
attrs = obj.attrs  # 得到标签内的属性数据，包括所有的属性，例如href...返回值是一个字典
obj.has_attr("id")  # 是否有某一个属性

tags = soup.find("body").children  # 找到内部的子标签（不是后代是子）
# 生成一个迭代器，需要一个个取值，此时换行也算一个值，但是换行\n不是Tag类型

tags_two = soup.find("body").descendants  # 查找子子孙孙标签，及其内部内容
soup.find("body").clear()  # 清除选定标签内部内容，但是保留标签名
soup.find("body").decompose()  # 清除选定标签内部内容，不保留标签名
tag = soup.find("body").extract()  # 提取某一个标签,会有一个返回值
```

## request.request()内部参数

` method`：post,put,patch,get,....

`url`

`params`：在url中传递的参数，params为字典，request模块会自动将参数接在url后面（?k1=v1&k2=v2）

`data`：在请求体内传递的数据，Dictionary, list of tuples, bytes, or file-like

```python
data = {"k1":"v1","k2":"v2"}
# data内容提交时，不能字典套字典，否则只会把字典的key传走
# 使用data进行数据传送时，请求头会增加一个键值对和请求体分别为：
"""
请求头：
conent-type = application/url-form-encoded

请求体：
k1=v1&k2=v2
"""
```

`json`：在请求体内传递的数据，  

```python
data = {"k1":"v1","k2":"v2"}
# 使用data进行数据传送时，请求头会增加一个键值对和请求体分别为：
"""
请求头：
conent-type = application/json

请求体：
"{"k1":"v1","k2":"v2"}"
"""
```

`headers`：请求头设置

# 轮询和长轮询

轮询代码简单，但是使用的服务器资源多

长轮询代码较为复杂，但是使用资源少

# 自定义异步IO

```python
import socket
import select


# client = socket.socket()
# client.connect(("www.baidu.com", 80))
# print("connected")
#
# # 发送二进制符合http协议的数据
# client.send(b"GET / HTTP/1.0\r\n Host: www.baidu.com\r\n\r\n")
# response_data = client.recv(8096)
# print(response_data)
#
# client.close()

# class HttpRequest:
#     def __init__(self, client, host):
#         self.socket = client
#         self.host = host
#
#     def fileno(self):
#         return self.socket.fileno()
#
#
# class AsyncRequest:
#     def __init__(self):
#         self.conn = []
#         self.connection = []  # 用来检测是否连接成功
#
#     def add_request(self, host):
#         try:
#             client = socket.socket()
#             client.setblocking(False)
#             client.connect((host, 80))
#         except BlockingIOError as e:
#             pass
#
#         request = HttpRequest(client, host)
#         self.conn.append(request)
#         self.connection.append(request)
#
#     def run(self):
#         while True:
#             rlist, wlist, elist = select.select(self.conn, self.connection, self.conn, 0.05)
#             for wItem in wlist:
#                 # 只要能够进入循环，代表和socket服务端连接成功
#                 print(wItem.host, "connected...")
#                 tql = "GET / HTTP/1.0\r\n Host: %s\r\n\r\n" %(wItem.host)
#                 wItem.socket.send(bytes(tql, encoding='utf-8'))
#                 self.connection.remove(wItem)  # 连接成功时从connection中移除
#
#             for rItem in rlist:
#                 recv_data = bytes()
#                 while True:
#                     try:
#                         chunk = rItem.socket.recv(8096)
#                         recv_data += chunk
#                     except Exception as e:
#                         break
#                 print(rItem.host, 'backData is ', recv_data)
#                 rItem.socket.close()
#                 self.conn.remove(rItem)
#
#             if len(self.conn) == 0:
#                 break
#
#
# url_list = [
#     "www.baidu.com",
#     "cn.bing.com",
# ]
#
# req = AsyncRequest()
# for url in url_list:
#     req.add_request(url)
#
# req.run()


class HttpRequest:
    def __init__(self, client, host, callback):
        self.socket = client
        self.host = host
        self.callback = callback

    def fileno(self):
        return self.socket.fileno()


class AsyncRequest:
    def __init__(self):
        self.conn = []
        self.connection = []  # 用来检测是否连接成功

    def add_request(self, host, callback):
        try:
            client = socket.socket()
            client.setblocking(False)
            client.connect((host, 80))
        except BlockingIOError as e:
            pass

        request = HttpRequest(client, host, callback)
        self.conn.append(request)
        self.connection.append(request)

    def run(self):
        while True:
            rlist, wlist, elist = select.select(self.conn, self.connection, self.conn, 0.05)
            for wItem in wlist:
                # 只要能够进入循环，代表和socket服务端连接成功
                print(wItem.host, "connected...")
                tql = "GET / HTTP/1.0\r\n Host: %s\r\n\r\n" %(wItem.host)
                wItem.socket.send(bytes(tql, encoding='utf-8'))
                self.connection.remove(wItem)  # 连接成功时从connection中移除

            for rItem in rlist:
                recv_data = bytes()
                while True:
                    try:
                        chunk = rItem.socket.recv(8096)
                        recv_data += chunk
                    except Exception as e:
                        break

                print(rItem.host, 'backData is ', recv_data)
                rItem.callback(recv_data)  # 数据处理

                rItem.socket.close()
                self.conn.remove(rItem)

            if len(self.conn) == 0:
                break


def function_baidu(recv_data):
    pass


def function_blog(recv_data):
    pass


if __name__ == '__main__':
    url_list = [
        {"host": "www.baidu.com", "callback": function_baidu},
        {"host": "cn.bing.com", "callback": function_blog},
    ]
    req = AsyncRequest()
    for url in url_list:
        req.add_request(url.get('host'), url.get('host'))
    req.run()
```

# 分割请求头、请求体

```python
headers, body = recv_data.split(b'\r\n\r\n', 1)
```

# Scrapy框架

<img src="https://upload-images.jianshu.io/upload_images/14946794-c32c1356bb94c85c.jpg?imageMogr2/auto-orient/strip|imageView2/2/format/webp" alt="img" style="zoom:80%;" />



 Spiders：爬虫，定义了爬取的逻辑和网页内容的解析规则，主要负责解析响应并生成结果和新的请求
 Engine：引擎，处理整个系统的数据流处理，出发事物，框架的核心
 Scheduler：调度器，接受引擎发过来的请求，并将其加入队列中，在引擎再次请求时将请求提供给引擎
 Downloader：下载器，下载网页内容，并将下载内容返回给spider
 ItemPipeline：项目管道，负责处理spider从网页中抽取的数据，主要是负责清洗，验证和向数据库中存储数据
 Downloader Middlewares：下载中间件，是处于Scrapy的Request和Response之间的处理模块
 Spider Middlewares：spider中间件，主要处理spider输入的响应和输出的结果及新的请求middlewares.py里实现



![img](https://upload-images.jianshu.io/upload_images/14946794-37efd25b5e187773.JPG)



- spider的yeild将request发送给engine，engine对request不做任何处理发送给scheduler
- scheduler生成request交给engine，engine拿到request，通过middleware发送给downloader
- downloader下载之后获取到数据并封装为response对象之后，又经过middleware发送给engine
- engine返回给spider，spider的parse()方法对获取到的response进行处理，解析出items或者requests
- Spider将解析出来的items或者requests发送给engine
- engine获取到items或者requests，将items发送给ItemPipeline，将requests发送给scheduler
- ItemPipeline将数据进行进一步处理，和数据库进行交互，数据持久化
- scheduler持续进行请求

## 使用

指定初始url

解析响应内容

## 基本命令

```
1. scrapy startproject 项目名称
   - 在当前目录中创建中创建一个项目文件（类似于Django）
 
2. scrapy genspider [-t template] <name> <domain>
   - 创建爬虫应用
   如：
      scrapy gensipider -t basic oldboy oldboy.com
      scrapy gensipider -t xmlfeed autohome autohome.com.cn
   PS:
      查看所有命令：scrapy gensipider -l
      查看模板命令：scrapy gensipider -d 模板名称
 
3. scrapy list
   - 展示爬虫应用列表
 
4. scrapy crawl 爬虫应用名称
   - 运行单独爬虫应用
```

## 基本文件

- scrapy.cfg  项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）
- items.py   设置数据存储模板，用于结构化数据，如：Django的Model
- pipelines   数据处理行为，如：一般结构化的数据持久化
- settings.py 配置文件，如：递归的层数、并发数，延迟下载等
- spiders    爬虫目录，如：创建文件，编写爬虫规则

## 第一次简单实用

```python
def parse(self, response):
    # content = str(response.body, encoding = 'utf-8')
    
    # 找到文档中所有的A标签
    # a = Selector(response = response).xpath('//a')
    # for i in a:
    #     print(i)
        
    # 将对象转换为字符串
    # a = Selector(response=response).xpath('//div[@od=content-list]/div[@class='item']').extract()
    
# 选择器:
"""
// 表示子孙中
.// 表示对象的子孙中
/ 儿子
/div 儿子中的div标签
//div 子孙中的div标签
/div[@id='idone'] 儿子中的div标签，且id=idone
obj.extract()	列表中的的每一个对象转换为字符串=》列表
obj.extract_first()	列表中的每一个对象转换为字符串=》列表第一个元素
//div/text()	获取某个标签的文本
"""
```

## 笔趣阁实例

### biquge.py

```python
import scrapy
from scrapy.selector import Selector
from scrapy.http import Response,Request
import sys
import io
import hashlib
from ..items import Novel


class BiqugeSpider(scrapy.Spider):
    # 使用set集合进行去重，实际上scrapy内部已经帮我们做了,需要自己配置
    # visited_url = set()
    name = 'biquge'
    # 规定爬取的域，这样有外部链接时不会爬取
    allowed_domains = ['www.xbiquge.la']
    start_urls = ['http://www.xbiquge.la/']

    # 可以重写start_requests处理起始的处理函数
    # def start_requests(self):
    #     for url in self.start_urls:
    #         yield Request(url, callback= self.parse)

    def parse(self, response):
        # response内部封装了很多内容
        # print(response.meta)     
        # meta是一个字典  
        # {'download_timeout': 180.0, 'download_slot': 'www.xbiquge.la', 'download_latency': 0.19447731971740723, 'depth': 0}
        # 
        # response.url  返回response的url
        # response.body
        # response.text

        novel_links = Selector(response=response).xpath("//div[@class='l']//li//span[@class='s2']")
        for item in novel_links:
            title = item.xpath(".//a/text()").extract_first().strip()
            href = item.xpath(".//a/@href").extract_first().strip()
            # print(title, "======>", href)

            novel_obj = Novel(title = title, href = href)
            # 将novel_obj传递给pipelines持久化，也可以打开文件，将对象写进去
            yield novel_obj

        
        # novel_links2 = Selector(response=response).xpath("//div[@class='l']//li//span[@class='s2']/a/@href").extract()
        # 驱虫的步骤实际上scrapy内部已经帮我们做了
        # for novel_link in novel_links:
        #     md5_url = self.md5(novel_link)
        #     if md5_url in self.visited_url:
        #         pass
        #     else:
        #         self.visited_url.add(md5_url)

        #         # 对url进行请求（交给调度器）yield一定要写，会被引擎自动识别
        #         yield Request(url=novel_link, callback=self.parse)

        #         print(novel_link)


# 一般在进行url的比较时，需要将其加密来使得某些较长的url保持较短的长度
# 这个道理也被用于数据库中的url的存储，通过加密保存相等长度的字符串，节省空间
    # def md5(self, url):
    #     obj = hashlib.md5()
    #     obj.update(bytes(url, encoding='utf-8'))
    #     return obj.hexdigest()


```

### pipelines.py+settings设置

```python
from itemadapter import ItemAdapter

class Study02Pipeline:
    def open_spider(self, spider):
        '''
        爬虫运行时会触发
        '''
        print("Spider opened...")

    def process_item(self, item, spider):
        # print(spider.__dict__)
        # if spider == 'biquge':
        print(item)
        tmp = "%s\n%s\n\n" %(item["title"], item["href"])
        with open('novel.txt','a', encoding='utf-8') as file:
            file.write(tmp)
        
        # return的item是交给下一个pipeline，如果你不想将item交给下一个pipeline
        # 一定要raise一个异常为：raise DropItem(),后期在扩展时可以监听异常抛出 
        return item

    def close_spider(self, spider):
        '''
        爬虫关闭时会触发
        '''
        print("Spider closed...")

class Study02Pipeline2:
    def process_item(self, item, spider):
        # print(item)
        return item
       
# settings设置
ITEM_PIPELINES = {
        # 可以有多个pipeline，数字越小越先执行
        'study_02.pipelines.Study02Pipeline': 300,
        'study_02.pipelines.Study02Pipeline2': 200,
    }
```

### items.py

```python
import scrapy


class Novel(scrapy.Item):
    title = scrapy.Field()
    href = scrapy.Field()
```

## settings参数

```python
# 1. 爬虫名称
BOT_NAME = 'step8_king'

# 2. 爬虫应用路径
SPIDER_MODULES = ['step8_king.spiders']
NEWSPIDER_MODULE = 'step8_king.spiders'

# Crawl responsibly by identifying yourself (and your website) on the user-agent
# 3. 客户端 user-agent请求头
# USER_AGENT = 'step8_king (+http://www.yourdomain.com)'

# Obey robots.txt rules
# 4. 禁止爬虫配置
# ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
# 5. 并发请求数
# CONCURRENT_REQUESTS = 4

# Configure a delay for requests for the same website (default: 0)
# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
# 6. 延迟下载秒数
# DOWNLOAD_DELAY = 2


# The download delay setting will honor only one of:
# 7. 单域名访问并发数，并且延迟下次秒数也应用在每个域名
# CONCURRENT_REQUESTS_PER_DOMAIN = 2
# 单IP访问并发数，如果有值则忽略：CONCURRENT_REQUESTS_PER_DOMAIN，并且延迟下次秒数也应用在每个IP
# CONCURRENT_REQUESTS_PER_IP = 3

# Disable cookies (enabled by default)
# 8. 是否支持cookie，cookiejar进行操作cookie
# COOKIES_ENABLED = True
# COOKIES_DEBUG = True

# Disable Telnet Console (enabled by default)
# 9. Telnet用于查看当前爬虫的信息，操作爬虫等...
#    使用telnet ip port ，然后通过命令操作
# TELNETCONSOLE_ENABLED = True
# TELNETCONSOLE_HOST = '127.0.0.1'
# TELNETCONSOLE_PORT = [6023,]


# 10. 默认请求头
# Override the default request headers:
# DEFAULT_REQUEST_HEADERS = {
#     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#     'Accept-Language': 'en',
# }


# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
# 11. 定义pipeline处理请求
# ITEM_PIPELINES = {
#    'step8_king.pipelines.JsonPipeline': 700,
#    'step8_king.pipelines.FilePipeline': 500,
# }



# 12. 自定义扩展，基于信号进行调用
# Enable or disable extensions
# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html
# EXTENSIONS = {
#     # 'step8_king.extensions.MyExtension': 500,
# }


# 13. 爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度
# DEPTH_LIMIT = 3

# 14. 爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo

# 后进先出，深度优先
# DEPTH_PRIORITY = 0
# SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'
# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'
# 先进先出，广度优先

# DEPTH_PRIORITY = 1
# SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'
# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'

# 15. 调度器队列
# SCHEDULER = 'scrapy.core.scheduler.Scheduler'
# from scrapy.core.scheduler import Scheduler


# 16. 访问URL去重
# DUPEFILTER_CLASS = 'step8_king.duplication.RepeatUrl'


# Enable and configure the AutoThrottle extension (disabled by default)
# See http://doc.scrapy.org/en/latest/topics/autothrottle.html

"""
17. 自动限速算法
    from scrapy.contrib.throttle import AutoThrottle
    自动限速设置
    1. 获取最小延迟 DOWNLOAD_DELAY
    2. 获取最大延迟 AUTOTHROTTLE_MAX_DELAY
    3. 设置初始下载延迟 AUTOTHROTTLE_START_DELAY
    4. 当请求下载完成后，获取其"连接"时间 latency，即：请求连接到接受到响应头之间的时间
    5. 用于计算的... AUTOTHROTTLE_TARGET_CONCURRENCY
    target_delay = latency / self.target_concurrency
    new_delay = (slot.delay + target_delay) / 2.0 # 表示上一次的延迟时间
    new_delay = max(target_delay, new_delay)
    new_delay = min(max(self.mindelay, new_delay), self.maxdelay)
    slot.delay = new_delay
"""

# 开始自动限速
# AUTOTHROTTLE_ENABLED = True
# The initial download delay
# 初始下载延迟
# AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
# 最大下载延迟
# AUTOTHROTTLE_MAX_DELAY = 10
# The average number of requests Scrapy should be sending in parallel to each remote server
# 平均每秒并发数
# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0

# Enable showing throttling stats for every response received:
# 是否显示
# AUTOTHROTTLE_DEBUG = True

# Enable and configure HTTP caching (disabled by default)
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings


"""
18. 启用缓存
    目的用于将已经发送的请求或相应缓存下来，以便以后使用
    
    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware
    from scrapy.extensions.httpcache import DummyPolicy
    from scrapy.extensions.httpcache import FilesystemCacheStorage
"""
# 是否启用缓存策略
# HTTPCACHE_ENABLED = True

# 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可
# HTTPCACHE_POLICY = "scrapy.extensions.httpcache.DummyPolicy"
# 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略
# HTTPCACHE_POLICY = "scrapy.extensions.httpcache.RFC2616Policy"

# 缓存超时时间
# HTTPCACHE_EXPIRATION_SECS = 0

# 缓存保存路径
# HTTPCACHE_DIR = 'httpcache'

# 缓存忽略的Http状态码
# HTTPCACHE_IGNORE_HTTP_CODES = []

# 缓存存储的插件
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'


"""
19. 代理，需要在环境变量中设置
    from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware
    
    方式一：使用默认
        os.environ
        {
            http_proxy:http://root:woshiniba@192.168.11.11:9999/
            https_proxy:http://192.168.11.11:9999/
        }
    方式二：使用自定义下载中间件
    
    def to_bytes(text, encoding=None, errors='strict'):
        if isinstance(text, bytes):
            return text
        if not isinstance(text, six.string_types):
            raise TypeError('to_bytes must receive a unicode, str or bytes '
                            'object, got %s' % type(text).__name__)
        if encoding is None:
            encoding = 'utf-8'
        return text.encode(encoding, errors)
        
    class ProxyMiddleware(object):
        def process_request(self, request, spider):
            PROXIES = [
                {'ip_port': '111.11.228.75:80', 'user_pass': ''},
                {'ip_port': '120.198.243.22:80', 'user_pass': ''},
                {'ip_port': '111.8.60.9:8123', 'user_pass': ''},
                {'ip_port': '101.71.27.120:80', 'user_pass': ''},
                {'ip_port': '122.96.59.104:80', 'user_pass': ''},
                {'ip_port': '122.224.249.122:8088', 'user_pass': ''},
            ]
            proxy = random.choice(PROXIES)
            if proxy['user_pass'] is not None:
                request.meta['proxy'] = to_bytes（"http://%s" % proxy['ip_port']）
                encoded_user_pass = base64.encodestring(to_bytes(proxy['user_pass']))
                request.headers['Proxy-Authorization'] = to_bytes('Basic ' + encoded_user_pass)
                print "**************ProxyMiddleware have pass************" + proxy['ip_port']
            else:
                print "**************ProxyMiddleware no pass************" + proxy['ip_port']
                request.meta['proxy'] = to_bytes("http://%s" % proxy['ip_port'])
    
    DOWNLOADER_MIDDLEWARES = {
       'step8_king.middlewares.ProxyMiddleware': 500,
    }
    
"""

"""
20. Https访问
    Https访问时有两种情况：
    1. 要爬取网站使用的可信任证书(默认支持)
        DOWNLOADER_HTTPCLIENTFACTORY = "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"
        DOWNLOADER_CLIENTCONTEXTFACTORY = "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory"
        
    2. 要爬取网站使用的自定义证书
        DOWNLOADER_HTTPCLIENTFACTORY = "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"
        DOWNLOADER_CLIENTCONTEXTFACTORY = "step8_king.https.MySSLFactory"
        
        # https.py
        from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory
        from twisted.internet.ssl import (optionsForClientTLS, CertificateOptions, PrivateCertificate)
        
        class MySSLFactory(ScrapyClientContextFactory):
            def getCertificateOptions(self):
                from OpenSSL import crypto
                v1 = crypto.load_privatekey(crypto.FILETYPE_PEM, open('/Users/wupeiqi/client.key.unsecure', mode='r').read())
                v2 = crypto.load_certificate(crypto.FILETYPE_PEM, open('/Users/wupeiqi/client.pem', mode='r').read())
                return CertificateOptions(
                    privateKey=v1,  # pKey对象
                    certificate=v2,  # X509对象
                    verify=False,
                    method=getattr(self, 'method', getattr(self, '_ssl_method', None))
                )
    其他：
        相关类
            scrapy.core.downloader.handlers.http.HttpDownloadHandler
            scrapy.core.downloader.webclient.ScrapyHTTPClientFactory
            scrapy.core.downloader.contextfactory.ScrapyClientContextFactory
        相关配置
            DOWNLOADER_HTTPCLIENTFACTORY
            DOWNLOADER_CLIENTCONTEXTFACTORY

"""



"""
21. 爬虫中间件
    class SpiderMiddleware(object):

        def process_spider_input(self,response, spider):
            '''
            下载完成，执行，然后交给parse处理
            :param response: 
            :param spider: 
            :return: 
            '''
            pass
    
        def process_spider_output(self,response, result, spider):
            '''
            spider处理完成，返回时调用
            :param response:
            :param result:
            :param spider:
            :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)
            '''
            return result
    
        def process_spider_exception(self,response, exception, spider):
            '''
            异常调用
            :param response:
            :param exception:
            :param spider:
            :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline
            '''
            return None
    
    
        def process_start_requests(self,start_requests, spider):
            '''
            爬虫启动时调用
            :param start_requests:
            :param spider:
            :return: 包含 Request 对象的可迭代对象
            '''
            return start_requests
    
    内置爬虫中间件：
        'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,
        'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,
        'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,
        'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,
        'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,

"""
# from scrapy.contrib.spidermiddleware.referer import RefererMiddleware
# Enable or disable spider middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html
SPIDER_MIDDLEWARES = {
   # 'step8_king.middlewares.SpiderMiddleware': 543,
}


"""
22. 下载中间件
    class DownMiddleware1(object):
        def process_request(self, request, spider):
            '''
            请求需要被下载时，经过所有下载器中间件的process_request调用
            :param request:
            :param spider:
            :return:
                None,继续后续中间件去下载；
                Response对象，停止process_request的执行，开始执行process_response
                Request对象，停止中间件的执行，将Request重新调度器
                raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception
            '''
            pass
    
    
    
        def process_response(self, request, response, spider):
            '''
            spider处理完成，返回时调用
            :param response:
            :param result:
            :param spider:
            :return:
                Response 对象：转交给其他中间件process_response
                Request 对象：停止中间件，request会被重新调度下载
                raise IgnoreRequest 异常：调用Request.errback
            '''
            print('response1')
            return response
    
        def process_exception(self, request, exception, spider):
            '''
            当下载处理器(download handler)或 process_request() (下载中间件)抛出异常
            :param response:
            :param exception:
            :param spider:
            :return:
                None：继续交给后续中间件处理异常；
                Response对象：停止后续process_exception方法
                Request对象：停止中间件，request将会被重新调用下载
            '''
            return None

    
    默认下载中间件
    {
        'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,
        'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,
        'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,
        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,
        'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,
        'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,
        'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,
        'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,
        'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,
        'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,
        'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,
        'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,
        'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,
        'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,
    }

"""
# from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware
# Enable or disable downloader middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
# DOWNLOADER_MIDDLEWARES = {
#    'step8_king.middlewares.DownMiddleware1': 100,
#    'step8_king.middlewares.DownMiddleware2': 500,
# }
```

## 中间件

### 爬虫中间件

```python
class SpiderMiddleware(object):

    def process_spider_input(self,response, spider):
        """
        下载完成，执行，然后交给parse处理
        :param response: 
        :param spider: 
        :return: 
        """
        pass

    def process_spider_output(self,response, result, spider):
        """
        spider处理完成，返回时调用
        :param response:
        :param result:
        :param spider:
        :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)
        """
        return result

    def process_spider_exception(self,response, exception, spider):
        """
        异常调用
        :param response:
        :param exception:
        :param spider:
        :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline
        """
        return None


    def process_start_requests(self,start_requests, spider):
        """
        爬虫启动时调用
        :param start_requests:
        :param spider:
        :return: 包含 Request 对象的可迭代对象
        """
        return start_requests
```

### 下载中间件

```python
class DownMiddleware1(object):
    def process_request(self, request, spider):
        """
        请求需要被下载时，经过所有下载器中间件的process_request调用
        :param request: 
        :param spider: 
        :return:  
            None,继续后续中间件去下载；
            Response对象，停止process_request的执行，开始执行process_response
            Request对象，停止中间件的执行，将Request重新调度器
            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception
        """
        pass

    def process_response(self, request, response, spider):
        """
        spider处理完成，返回时调用
        :param response:
        :param result:
        :param spider:
        :return: 
            Response 对象：转交给其他中间件process_response
            Request 对象：停止中间件，request会被重新调度下载
            raise IgnoreRequest 异常：调用Request.errback
        """
        print('response1')
        return response

    def process_exception(self, request, exception, spider):
        """
        当下载处理器(download handler)或 process_request() (下载中间件)抛出异常
        :param response:
        :param exception:
        :param spider:
        :return: 
            None：继续交给后续中间件处理异常；
            Response对象：停止后续process_exception方法
            Request对象：停止中间件，request将会被重新调用下载
        """
        return None
```

## URL去重

scrapy默认使用 scrapy.dupefilter.RFPDupeFilter 进行去重，相关配置为

```python
DUPEFILTER_CLASS = 'scrapy.dupefilter.RFPDupeFilter'
DUPEFILTER_DEBUG = False
JOBDIR = "保存范文记录的日志路径，如：/root/"  # 最终路径为 /root/requests.seen
```

### 自定义URL去重

```python
class RepeatUrl:
    def __init__(self):
        self.visited_url = set()

    @classmethod
    def from_settings(cls, settings):
        """
        初始化时，调用
        :param settings: 
        :return: 
        """
        return cls()

    def request_seen(self, request):
        """
        检测当前请求是否已经被访问过
        :param request: 
        :return: True表示已经访问过；False表示未访问过
        """
        if request.url in self.visited_url:
            return True
        self.visited_url.add(request.url)
        return False

    def open(self):
        """
        开始爬去请求时，调用
        :return: 
        """
        print('open replication')

    def close(self, reason):
        """
        结束爬虫爬取时，调用
        :param reason: 
        :return: 
        """
        print('close replication')

    def log(self, request, spider):
        """
        记录日志
        :param request: 
        :param spider: 
        :return: 
        """
        print('repeat', request.url)

```

## 自定义扩展

自定义扩展时，利用信号在指定位置注册指定操作

```python
from scrapy import signals

class MyExtension(object):
    def __init__(self, value):
        self.value = value

    @classmethod
    def from_crawler(cls, crawler):
        val = crawler.settings.getint('MMMM')
        ext = cls(val)

        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)

        return ext

    def spider_opened(self, spider):
        print('open')

    def spider_closed(self, spider):
        print('close')
```

# Scrapy原理

